<!DOCTYPE html>

<html lang="es">
<head>
<meta charset="utf-8"/>
<title>
   Translation Part 46 - Translated (es/) - Part 46
  </title>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta content="" name="language"/>
<meta content="es" name="sourcelang"/>
<meta content="General" name="topic"/>
<meta content="noindex" name="robots"/>
<link href="../style.css" rel="stylesheet"/>
<style>
   .row { display: flex; border-bottom: 1px solid #eee; padding: 10px 0; }
    .col1, .col2 { flex: 1; padding: 0 10px; }
    .col2 em { color: #555; }
    #header { margin-bottom: 2em; border-bottom: 2px solid #000; padding-bottom: 10px; }
    #header h1 { font-size: 1.8em; margin-bottom: 0.2em; }
    #wordcount, #part { font-style: italic; color: #666; font-size: 0.95em; }
    @media (max-width: 768px) {
      .row { flex-direction: column; }
      .col1, .col2 { padding: 5px 0; }
    }
  </style>
</head>
<body>
<div id="header">
<h1>
    Translation Part 46
   </h1>
<p id="wordcount">
    Word Count (Part): 2010
   </p>
<p id="part">
    Part 46
   </p>

</div>
<div id="sentence-table">
<div class="row">
<div class="col1">
<p>
      Una regresión de Cox es una forma de regresión múltiple en la que la variable de respuesta es el tiempo de supervivencia, y los coeficientes corresponden a log(razones de riesgo).
     </p>
</div>
<div class="col2">
<p>
      A Cox regression is a multiple regression form in which the response variable is survival time, and coefficients correspond to Log (risk reasons).
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      razón de tasas de incidencia: incremento relativo en el número esperado de eventos en un período fijo de tiempo asociado con una exposición.
     </p>
</div>
<div class="col2">
<p>
      Incidence rates ratio: relative increase in the expected number of events in a fixed period of time associated with an exhibition.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Una regresión Poisson es una forma de regresión múltiple en la que la variable dependiente es la tasa observada, y los coeficientes corresponden a log(razones de tasas).
     </p>
</div>
<div class="col2">
<p>
      A Poisson regression is a multiple form of regression in which the dependent variable is the observed rate, and the coefficients correspond to Log (rates reasons).
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      razón de verosimilitud: medida del apoyo relativo que unos datos proporcionan a dos hipótesis rivales.
     </p>
</div>
<div class="col2">
<p>
      Verosimilitude reason: measure of relative support that data provides to two rival hypotheses.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Para las hipótesis H0 y H1, la razón de verosimilitud proporcionada por los datos x viene dada por p(x|H0) / p(x|H1).
     </p>
</div>
<div class="col2">
<p>
      For H0 and H1 hypotheses, the background reason provided by data X is given by P (x | h0) / p (x | h1).
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      regresión a la media: observación alta o baja que es seguida por otra que es menos extrema por un proceso de variación natural.
     </p>
</div>
<div class="col2">
<p>
      Regression to the average: high or low observation that is followed by another that is less extreme by a natural variation process.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Ocurre porque parte de la explicación del caso extremo inicial es el azar, y es poco probable que se repita en la misma medida.
     </p>
</div>
<div class="col2">
<p>
      It occurs because part of the explanation of the initial extreme case is chance, and it is unlikely to be repeated to the same extent.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      regresión de Cox: véase razón de riesgo.
     </p>
</div>
<div class="col2">
<p>
      Cox regression: See Risk Reason.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      regresión lineal múltiple: supongamos que para cada respuesta yi hay un conjunto p de variables independientes (xi1, xi2 … xip).
     </p>
</div>
<div class="col2">
<p>
      Multiple linear regression: Suppose that for each response YI there is a set of independent variables (XI1, Xi2 ... Xip).
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Entonces, una regresión lineal múltiple de mínimos cuadrados viene dada por: ŷi = b0 + b1(xi1 − x1) + b2(xi2 − x2) + … + bp(xip − xp), donde los coeficientes b0, b1 … bp son escogidos para minimizar la suma cuadrática residual SCR = i(yi − ŷi)2.
     </p>
</div>
<div class="col2">
<p>
      Then, a multiple linear regression of minimum squares is given by: ŷi = b0 + b1 (xi1 - x1) + b2 (xi2 - x2) +… + bp (xip - xp), where the coefficients b0, b1… bp are chosen to minimize the residual quadratic sum scr = i (yi - ŷ) 2.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      La constante b0 es simplemente la media y, mientras que la fórmula para los demás coeficientes es compleja pero fácil de calcular.
     </p>
</div>
<div class="col2">
<p>
      The B0 constant is simply the average and, while the formula for other coefficients is complex but easy to calculate.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Nótese que b0 = y es el valor predicho de una observación y cuyas variables Σindependientes son las medias (x1, x2 … xp), y al igual que en el caso de la regresión lineal, se obtiene una yi ajustada a partir de los residuos más la constante, o yi − ŷi + y.
     </p>
</div>
<div class="col2">
<p>
      Note that b0 = and is the predicted value of an observation and whose σinderent variables are the stockings (x1, x2… xp), and as in the case of linear regression, a adjusted yi is obtained from the waste plus the constant, or yi - ŷi + y.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      regresión logística: forma de regresión múltiple en la que la variable de respuesta es una proporción, y los coeficientes corresponden a log(razones de probabilidad).
     </p>
</div>
<div class="col2">
<p>
      Logistic regression: multiple regression form in which the response variable is a proportion, and the coefficients correspond to Log (probability reasons).
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Supongamos que observamos una serie de proporciones yi = ri / ni, que se asume que se derivan de una variable binomial con una probabilidad subyacente pi con un correspondiente conjunto de variables independientes (xi1, xi2 … xip).
     </p>
</div>
<div class="col2">
<p>
      Suppose we observe a series of proportions YI = RI / Ni, which is assumed that they derive from a binomial variable with an underlying probability Pi with a corresponding set of independent variables (XI1, XI2 ... Xip).
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Se asume que el logaritmo de la razón de probabilidad pi es una regresión lineal: log pi 1 − p i = b0 + b1xi1 + b2xi2 + … + bpxip Supongamos que una de las variables independientes, por ejemplo x1, es binaria con x1 = 0, lo que se corresponde a no estar expuesta a un riesgo potencial, y x1 = 1, que corresponde a estar expuesta.
     </p>
</div>
<div class="col2">
<p>
      It is assumed that the logarithm of the probability ratio pi is a linear regression: log pi 1 - p i = b0 + b1xi + b2xi2 +… + bpxip suppose that one of the independent variables, for example x1, is binary with x1 = 0, which corresponds to not being exposed to a potential risk, and x1 = 1, which corresponds to being exposed.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Entonces el coeficiente b1 es un log(razón de probabilidad).
     </p>
</div>
<div class="col2">
<p>
      Then the B1 coefficient is a log (probability reason).
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      regresión multinivel y post-estratificación (RMP): moderno desarrollo en el campo de las muestras basadas en encuestas en el cual se obtiene un número singularmente pequeño de entrevistados de muchas áreas.
     </p>
</div>
<div class="col2">
<p>
      Multilevel regression and post-stratification (RMP): Modern development in the field-based sampling field in which a uniquely small number of interviewees from many areas are obtained.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Después se construye un modelo de regresión que relaciona las respuestas con factores demográficos, controlando por variabilidad adicional entre áreas mediante modelos jerárquicos.
     </p>
</div>
<div class="col2">
<p>
      Then a regression model is built that relates the responses to demographic factors, controlling by additional variability between areas through hierarchical models.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Si se conocen los datos demográficos de todas las áreas, se pueden hacer predicciones locales y nacionales, con la incertidumbre apropiada.
     </p>
</div>
<div class="col2">
<p>
      If the demographic data of all areas are known, local and national predictions can be made, with the appropriate uncertainty.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      residuo: diferencia entre un valor observado y el predicho por un modelo estadístico.
     </p>
</div>
<div class="col2">
<p>
      Waste: difference between an observed value and the predicted by a statistical model.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      riesgo absoluto: porcentaje de personas en un grupo determinado que experimenta un evento de interés en un período de tiempoespecífico.
     </p>
</div>
<div class="col2">
<p>
      Absolute risk: Percentage of people in a given group that experiences an event of interest in a period of time.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      riesgo relativo: si el riesgo absoluto entre personas que están expuestas a algo que nos interesa es p, y el riesgo absoluto entre personas que no están expuestas es q, entonces el riesgo relativo es p/q.
     </p>
</div>
<div class="col2">
<p>
      Relative risk: If the absolute risk between people who are exposed to something that interests us is P, and the absolute risk between people who are not exposed is q, then the relative risk is P/q.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      sabiduría de las multitudes: idea de que un resumen que se deriva de la opinión de un grupo está más cerca de la verdad que la opinión de la mayoría de los individuos.
     </p>
</div>
<div class="col2">
<p>
      Wisdom of the crowds: idea that a summary that derives from the opinion of a group is closer to the truth than the opinion of most individuals.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      sensibilidad: proporción de casos «positivos» que son correctamente identificados por un clasificador o prueba, a menudo denominada como tasa de verdaderos positivos.
     </p>
</div>
<div class="col2">
<p>
      Sensitivity: proportion of "positive" cases that are correctly identified by a classifier or proof, often called as true positive.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Uno menos la sensibilidad también se conoce como error observado de tipo II o tasa de falsos negativos.
     </p>
</div>
<div class="col2">
<p>
      One less sensitivity is also known as observed type II error or false negative rate.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      señal y ruido: idea de que los datos observados surgen de dos componentes: una señal determinista que es en lo que estamos realmente interesados, y un ruido aleatorio que comprende el error residual.
     </p>
</div>
<div class="col2">
<p>
      Signal and noise: idea that the observed data arise from two components: a deterministic signal that is in what we are really interested, and a random noise that includes the residual error.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      El desafío de la inferencia estadística es identificar apropiadamente ambos, y no pensar que el ruido es en realidad una señal.
     </p>
</div>
<div class="col2">
<p>
      The challenge of statistical inference is to appropriately identify both, and not think that noise is actually a signal.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      significatividad estadística: se considera que un efecto observado es estadísticamente significativo cuando su valor P correspondiente a una hipótesis nula es menor de algún nivel preespecificado, digamos 0,05 o 0,001, lo que significa que un resultado tan extremo es poco probable que ocurra si la hipótesis nula, y todas las otras asunciones de los modelos, se cumplen.
     </p>
</div>
<div class="col2">
<p>
      Statistical significance: It is considered that an observed effect is statistically significant when its value P corresponding to a null hypothesis is lower of some prescription level, say 0.05 or 0.001, which means that such an extreme result is unlikely to occur if the null hypothesis, and all other assumptions of the models, are fulfilled.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      significatividad práctica: propiedad del descubrimiento que tiene una importancia real.
     </p>
</div>
<div class="col2">
<p>
      Practical significance: Property of the discovery that is of real importance.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Hay estudios que pueden dar lugar a resultados estadísticamente significativos, pero sin significatividad práctica.
     </p>
</div>
<div class="col2">
<p>
      There are studies that can lead to statistically significant results, but without practical significance.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      sobreajuste: elaboración de un modelo estadístico que está sobreadaptado al entrenamiento de los datos, de manera que su capacidad predictiva comienza a disminuir.
     </p>
</div>
<div class="col2">
<p>
      OVERAJUSTE: ELABORATION OF A STATISTIC MODEL THAT IS OVERDERED TO DATA TRAINING, SO THAT THEIR PREDICTIVE CAPACITY BEGINS TO DECREASE.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      solución de compromiso sesgo-varianza: cuando se ajusta un modelo para predecir, una creciente complejidad llevaráfinalmente a un modelo que tiene menos sesgo, en el sentido de más potencial de adaptarse a los detalles del proceso subyacente, pero más varianza, dado que no hay suficientes datos para confiar en los parámetros del modelo.
     </p>
</div>
<div class="col2">
<p>
      Commitment solution bias-variance: When a model is adjusted to predict, a growing complexity will take a model that has less bias, in the sense of more potential to adapt to the details of the underlying process, but more variance, since there are not enough data to trust the parameters of the model.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Tiene que llegarse a un compromiso entre estos elementos para evitar el sobreajuste.
     </p>
</div>
<div class="col2">
<p>
      It has to reach a commitment between these elements to avoid the overhap.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      tasa de descubrimientos falsos: al comprobar múltiples hipótesis, porcentaje de afirmaciones positivas que resultan ser falsos positivos.
     </p>
</div>
<div class="col2">
<p>
      False discoveries rate: when checking multiple hypotheses, percentage of positive statements that turn out to be false positives.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      teorema central del límite: tendencia de que la media de la muestra de un conjunto de variables aleatorias tenga una distribución muestral normal, con independencia (con ciertas excepciones) de la forma de la distribución muestral subyacente de la variable aleatoria.
     </p>
</div>
<div class="col2">
<p>
      Central theorem of the limit: tendency that the average of the sample of a set of random variables has a normal sample distribution, regardless (with certain exceptions) of the form of the underlying sample distribution of the random variable.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Si cada n observaciones independientes tienen media μ y varianza σ2, entonces bajo unas asunciones amplias, su media muestral es un estimador de μ, y tiene una distribución aproximadamente normal con media μ, varianza σ2 / n, y desviación típica σ / √n (también conocida como error típico del estimador).
     </p>
</div>
<div class="col2">
<p>
      If each n independent observations have medium μ and variance σ2, then under broad assumptions, its sample average is a μ estimator, and has an approximately normal distribution with average μ, variance σ2 / n, and typical deviation σ / √n (also known as the typical error of the estimator).
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      teorema de Bayes: regla de probabilidad que muestra cómo la evidencia A actualiza las creencias previas de una proposición B para producir creencias posteriores P(B|A), a través de la fórmula P(B|A) = P(A|B)P(B)/P(A).
     </p>
</div>
<div class="col2">
<p>
      Bayes theorem: Probability rule that shows how evidenced to updates the previous beliefs of a proposition B to produce subsequent beliefs P (B | A), through formula P (B | a) = P (A | b) P (b)/p (a).
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Esto es fácil de demostrar: dado que P(B Y A) = P(A Y B), la regla de probabilidad de la multiplicación significa que P(B|A)P(A) = P(A|B)P(B), y, dividiendo cada lado por P(A), nos da el teorema.
     </p>
</div>
<div class="col2">
<p>
      This is easy to demonstrate: Since P (B and A) = P (A and B), the rule of probability of multiplication means that p (b | a) p (a) = p (a | b) p (b), and, dividing each side by p (a), gives us the theorem.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      test de asociación o bondad de ajuste chi-cuadrado: prueba estadística que indica el grado de incompatibilidad de los datos con un modelo estadístico supuesto que comprende la hipótesis nula, que puede indicar ausencia de asociación, u otra forma matemática.
     </p>
</div>
<div class="col2">
<p>
      Association or Chi-square adjustment benefit test: Statistical test indicating the degree of incompatibility of the data with a statistical model assumed that includes the null hypothesis, which may indicate absence of association, or other mathematical form.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Concretamente, la prueba compara un conjunto m de recuentos esperados o1, o2 … om con un conjunto de valores esperados e1, e2 … em que han sido calculados bajo la hipótesisnula.
     </p>
</div>
<div class="col2">
<p>
      Specifically, the test compares a set m of expected counts O1, O2 ... OM with a set of expected values E1, E2 ... Em that have been calculated under the hypothesis.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      La versión más sencilla de la prueba estadística es la siguiente: χ2 = j (oj − ej)2 ej Bajo la hipótesis nula χ2 tendrá aproximadamente una distribución muestral chi-cuadrada, lo que permite calcular un valor P asociado.
     </p>
</div>
<div class="col2">
<p>
      The simplest version of the statistical test is as follows: χ2 = J (OJ-ex) 2 Ex under the null hypothesis χ2 will have approximately a sample sample chi-square, allowing to calculate an associated P value.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      test de permutación-aleatorización: forma de prueba de hipótesis en la cual la distribución del estadístico de prueba bajo la hipótesis nula se obtiene permutando las etiquetas de los datos, en lugar de por medio de un detallado modelo estadístico para las variables aleatorias.
     </p>
</div>
<div class="col2">
<p>
      Permutation-German test: Hypothesis test form in which the distribution of the test statistic under the null hypothesis is obtained by swapping data labels, instead of a detailed statistical model for random variables.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Supongamos que la hipótesis nula es que una «etiqueta», como por ejemplo, ser hombre o mujer, no está asociada con un resultado.
     </p>
</div>
<div class="col2">
<p>
      Suppose the null hypothesis is that a "label", such as being a man or woman, is not associated with a result.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Las pruebas de aleatorización examinan todas las formas posibles por las cuales pueden ser reasignadas las etiquetas para puntos de datos individuales, cada uno de los cuales es igualmente probable bajo la hipótesis nula.
     </p>
</div>
<div class="col2">
<p>
      Randomization tests examine all possible ways by which labels for individual data points can be reallocated, each of which is equally probable under the null hypothesis.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Se calcula la prueba estadística para cada una de estas permutaciones, y el valor P viene dado por la proporción que lleva a pruebas estadísticas más extremas que las realmente observadas.
     </p>
</div>
<div class="col2">
<p>
      The statistical test for each of these permutations is calculated, and the value P is given by the proportion that leads to statistical tests more extreme than those really observed.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      validación cruzada: forma de evaluar la calidad de un algoritmo usado para la predicción o la clasificación eliminando sistemáticamente algunos casos para que sirvan de conjunto de entrenamiento.
     </p>
</div>
<div class="col2">
<p>
      Cross validation: way to evaluate the quality of an algorithm used for prediction or classification systematically eliminating some cases to serve as a training set.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      validez externa: cuando las conclusiones de un estudio son generalizables a un grupo objetivo, más amplio que la población inmediata que ha sido estudiada.
     </p>
</div>
<div class="col2">
<p>
      External validity: when the conclusions of a study are generalizable to an objective group, broader than the immediate population that has been studied.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Se refiere a la relevancia de un estudio.
     </p>
</div>
<div class="col2">
<p>
      It refers to the relevance of a study.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      validez interna: cuando las conclusiones de un estudio se aplican realmente a la población de un estudio.
     </p>
</div>
<div class="col2">
<p>
      Internal validity: When the conclusions of a study really apply to the population of a study.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Esto se refiere al rigor con el cual se ha realizado un estudio.
     </p>
</div>
<div class="col2">
<p>
      This refers to the rigor with which a study has been conducted.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Σvalor P: medida de la discrepancia entre los datos y la hipótesis nula.
     </p>
</div>
<div class="col2">
<p>
      Σvalor P: Measure of the discrepancy between the data and the null hypothesis.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Para una hipótesis nula H0, sea T el estadístico para el cual unos valores grandes indican inconsistencia con H0.
     </p>
</div>
<div class="col2">
<p>
      For a null H0 hypothesis, be it the statistic for which large values indicate inconsistency with H0.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Supongamos que observamos un valor-t.
     </p>
</div>
<div class="col2">
<p>
      Suppose we observe a value-t.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Entonces, un valor P (de una sola cola) es la probabilidad de observar un valor tan extremo, si H0 fuese cierta, es decir P(T ≥ t|H0).
     </p>
</div>
<div class="col2">
<p>
      Then, a P (one tail) value is the probability of observing such an extreme value, if it was true, that is P (T ≥ T | H0).
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Si tanto valores pequeños como grandes de T indican inconsistencia con H0, entonces el valor P de dos colas es la probabilidad de observar un valor tan grande en ambas direcciones.
     </p>
</div>
<div class="col2">
<p>
      If both small and large values of T indicate inconsistency with H0, then the value P of two tails is the probability of observing such a large value in both directions.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      A menudo se considera que el valor P de dos colas es simplemente el doble del valor P de una sola cola, mientras que el programa R usa la probabilidad total de eventos que tienen una probabilidad menor de ocurrencia que el realmente observado.
     </p>
</div>
<div class="col2">
<p>
      It is often considered that the two -tails P value is simply twice the P of a single tail, while the R program uses the total probability of events that have a lower probability of occurrence than the really observed.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      valores P de una y de dos colas: los correspondientes a pruebas unilaterales y bilaterales.
     </p>
</div>
<div class="col2">
<p>
      P values of one and two tails: those corresponding to unilateral and bilateral tests.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      variabilidad: diferencias inevitables que se producen entre las mediciones u observaciones, algunas de las cuales pueden ser explicadas por factores conocidos, mientras que las restantes son atribuidas a ruido aleatorio.
     </p>
</div>
<div class="col2">
<p>
      Variability: inevitable differences that occur between measurements or observations, some of which can be explained by known factors, while the remaining ones are attributed to random noise.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      variable aleatoria: cantidad que se asume que tiene una distribución de probabilidad.
     </p>
</div>
<div class="col2">
<p>
      Random variable: amount assumed that it has a probability distribution.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Antes de ser observadas, a las variables aleatorias se las nombra mediante una letra mayúscula X, mientras que los valores observados se denominan x.
     </p>
</div>
<div class="col2">
<p>
      Before being observed, the random variables are named by means of a capital letter X, while the observed values are called x.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      variable categórica: variable que puede tomar dos o más valores discretos, que pueden o no estar ordenados.
     </p>
</div>
<div class="col2">
<p>
      Categorical variable: Variable that can take two or more discrete values, which may or may not be ordered.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      variable contable: variable que puede tener valores enteros 0, 1, 2, y así sucesivamente.
     </p>
</div>
<div class="col2">
<p>
      Accounting variable: variable that can have entire values 0, 1, 2, and so on.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      variable continua: variable aleatoria X que puede, al menos en principio, adoptar cualquier valor dentro de un rango específico.
     </p>
</div>
<div class="col2">
<p>
      Continuous variable: random variable X that can, at least in principle, adopt any value within a specific range.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Tiene una función de probabilidad de densidad f tal quey una esperanza dada por La probabilidad de que X esté en el intervalo (A, B) puede calcularse usando variable de confusión: variable que está asociada tanto con la dependiente como con el predictor, y que puede explicar parte de su aparente relación.
     </p>
</div>
<div class="col2">
<p>
      It has a probability of density f such as a hope given by the probability that x be in the interval (a, b) can be calculated using confusion variable: variable that is associated with both the dependent and the predictor, and that it can explain part of its apparent relationship.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Por ejemplo, el peso y la estatura de los niños están marcadamente correlacionados, pero gran parte de esa asociación se explica por la edad del niño.
     </p>
</div>
<div class="col2">
<p>
      For example, the weight and height of children are markedly correlated, but much of that association is explained by the child's age.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      variable dependiente, de respuesta o de resultado: variable de interés principal que deseamos predecir o explicar.
     </p>
</div>
<div class="col2">
<p>
      Dependent variable, response or result: Variable of main interest that we want to predict or explain.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      variable independiente (predictor): variable que está fija por diseño u observación, y cuya asociación con una variable de resultado podría ser de interés.
     </p>
</div>
<div class="col2">
<p>
      Independent variable (predictor): variable that is fixed by design or observation, and whose association with a result variable could be of interest.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      varianza: para una muestra x1 … xn con media x, esto se define generalmente como s2 = (n − 1)−1 i (xi − x)2 (aunque el denominador puede también ser n en lugar de n − 1).
     </p>
</div>
<div class="col2">
<p>
      Variance: For a sample x1… xn with average x, this is generally defined as s2 = (n - 1) −1 i (xi - x) 2 (although the denominator can also be n instead of n - 1).
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Para una variable aleatoria X con media μ, la varianza es V(X) = E(X − μ)2.
     </p>
</div>
<div class="col2">
<p>
      For a random variable x with average μ, the variance is V (x) = E (x - μ) 2.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      La desviación típica es la raíz cuadrada de la varianza, DT(X) = √V(X).
     </p>
</div>
<div class="col2">
<p>
      The standard deviation is the square root of the variance, dt (x) = √v (x).
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      verosimilitud: medida del apoyo que los datos otorgan a valores particulares de los parámetros.
     </p>
</div>
<div class="col2">
<p>
      Verosimilitude: measure of the support granted to particular values of the parameters.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Cuando una distribución de probabilidad para una variable aleatoria depende de un parámetro, por ejemplo θ, entonces después de observar el dato x la verosimilitud de θ es proporcional a p(x|θ).
     </p>
</div>
<div class="col2">
<p>
      When a probability distribution for a random variable depends on a parameter, for example θ, then after observing the data x the likelihood of θ is proportional to P (x | θ).
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      ΣDAVID SPIEGELHALTER (Barnstaple, Devon, England, 1953), estadístico británico y miembro del Churchill College, Cambridge.
     </p>
</div>
<div class="col2">
<p>
      Σdavid Spiegelhalter (Barstaple, Devon, England, 1953), British statistic and member of the Churchill College, Cambridge.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      De 2007 a 2018 fue Profesor Winton de Comprensión Pública del Riesgo en el Laboratorio de Estadística de la Universidad de Cambridge.
     </p>
</div>
<div class="col2">
<p>
      From 2007 to 2018 he was Professor Winton of Public Understanding the Risk at the Statistics Laboratory at the University of Cambridge.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Spiegelhalter es un investigador altamente citado por el ISI.
     </p>
</div>
<div class="col2">
<p>
      Spiegelhalter is a researcher highly cited by the ISI.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      Es presidente del Centro Winton de Comunicación de Riesgos y Pruebas en el Centro de Ciencias Matemáticas de Cambridge.
     </p>
</div>
<div class="col2">
<p>
      He is president of the Winton Center for Risk and Tests Communication at the Cambridge Mathematical Sciences Center.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      El 27 de mayo de 2020 se incorporó al consejo de la Autoridad Estadística del Reino Unido como director no ejecutivo por un periodo de tres años.
     </p>
</div>
<div class="col2">
<p>
      On May 27, 2020, he joined the Council of the United Kingdom Statistical Authority as a non -executive director for a period of three years.
     </p>
</div>
</div>
<div class="row">
<div class="col1">
<p>
      En 2012 Spiegelhalter presentó el documental de la BBC Four Tails You Win: The Science of Chance, que describía la aplicación de la probabilidad en la vida cotidiana.
     </p>
</div>
<div class="col2">
<p>
      In 2012 Spiegelhalter presented the documentary of the BBC Four Tails You Win: The Science of Chance, which described the application of probability in everyday life.
     </p>
</div>
</div>
</div>
</body>
</html>
